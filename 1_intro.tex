\chapter{Introduction}
\label{chapter:intro}
Ultra-low-power (ULP) sensor devices are increasingly being deployed for a variety of use-cases from in-body health sensing to civil infrastructure monitoring to tiny chip-scale satellites~\cite{kicksat}.
% 
These devices collect data from their deployed environment and must process the 
data to support applications.
% 
Processing varies and may entail digital signal
processing (DSP), computing statistics, sorting, or sophisticated computations
such as machine learning (ML) inference using a deep neural network (DNN).
% 
As processing sophistication has increased, sensor device capability also
has matured to include high-definition image sensors~\cite{naderiparizi2018towards} and multi-sensor arrays~\cite{laput2017synthetic}, increasing sensed data volume.
%
This shift poses a challenge: how can we perform sophisticated computations
on simple, ultra-low-power systems?
% 
The answer --- a new system stack --- is the objective of this work.

Energy-efficiency is the key determinant of device and application performance.
% 
For devices powered by batteries, energy dictates lifetime and for devices harvesting energy from the environment, energy dictates performance by controlling the frequency of recharging.
% 
However, existing systems suffer fundamental inefficiencies.
% 
One solution is to offload computation, but communication costs more than compute.
% 
Another solution is to specialize hardware for a particular application, but specialization compromises on programmability and adaptability.
% 
The only approach, therefore, is to redesign the system stack --- from software to silicon --- to increase energy-efficiency while maintaining programmability.
% 
This work  contributes software, a compiler, computer architecture, and a silicon implementation towards this goal.
% 
The key insight is that new execution models can resolve the underlying tension between energy-efficiency and general-purpose programmability.

\section{Challenges}
There are a multitude of challenges to supporting sophisticated processing on ULP sensor devices.
% 
These devices are severely resource and energy-constrained, complicating application development and limiting the lifetimes of deployed devices.
% 
There is a need for extreme energy-efficiency without sacrificing programmability.
% 
This the central theme to the specific challenges detailed below.

\subsubsection{Offloading computation does not scale}
Offloading computation from a ULP device to a more powerful nearby computer (e.g., at the ``edge'' or cloud) is one approach to the increased processing sophistication and sensed data volume of applications in the ULP domain.
%
The more data a sensor produces, though, the more data the device must
communicate.
%
Unfortunately, transmitting data compromises security, clogs networks, and takes much more energy per byte than
sensing, storing, or locally computing on those data~\cite{sonic,zebranet}.  
%
While a high-powered device like a smartphone, with a high-bandwidth,
long-range radio, can afford to offload data to the edge or cloud,
this is not practical for power-, energy-, and bandwidth-limited sensor devices~\cite{dongare2017openchirp,sonic}.

\subsubsection{Local compute reduces cost of communication}
Since offloading is infeasible,
the alternative is to process data \emph{locally} on the sensor node itself.
% 
My work, \sonic, demonstrates how systems can use commodity off-the-shelf microcontrollers (COTS MCU) to filter sensed data locally so that only meaningful data (as defined by the application) are transmitted.
% 
Processing data locally minimizes the high energy cost of communication, reducing energy by $\approx20\times$ compared to a design that always offloads, but makes the application highly sensitive to the energy-efficiency of computation.

\subsubsection{Energy-efficiency is critical to end-to-end system performance}
Energy efficiency is the primary determinant of end-to-end system performance in ULP embedded systems.
% 
For battery-powered devices~\cite{culler2002mica,rowe2011sensor}, energy efficiency determines device lifetime: once a single-charge battery has been depleted the device is dead and it is impractical to replace the battery on millions of deployed devices.
% 
Even rechargeable batteries are limited in the number of recharge cycles, and a simple data-logging application can wear out the battery in just a few years~\cite{jackson_2019,nardello2019camaroptera}.
% 
For energy-harvesting devices~\cite{colin2018reconfigurable,hester2015tragedy,flicker,moo,windware}, energy efficiency determines device performance.
% 
These devices store energy in a capacitor and spend most of their time powered off, waiting for the capacitor to recharge.
% 
Greater energy efficiency leads to less time waiting and more time doing useful work~\cite{desai2020power}.

\subsubsection{Existing devices are energy-inefficient}
However, ULP COTS MCUs used in many deeply embedded sensor nodes
(e.g., TI MSP430, ARM M0+ \& M4+) are energy-inefficient.
%
These MCUs are general-purpose, programmable devices that support a variety of
applications.
% 
But this generality comes at at a high power, energy, and performance cost.

Programmability is expensive in two main ways~\cite{horowitz:isscc14:energy-keynote,hameed2010understanding,balfour_elm_thesis}.
First, \emph{instruction supply} consumes significant energy: in the best case, the energy of
an instruction cache hit, and in the worst case, the energy of a main memory
read and instruction cache fill.  
%
Lacking sophisticated microarchitectural features such as superscalar and
out-of-order execution pipelines~\cite{msp430fr5994,traber2016pulpino}, the energy overhead of
instruction supply constitutes a significant fraction of total operating energy.
%
Second, data supply through \emph{register file (RF) access} also consumes significant energy.
%
Together, instruction and data supply can consume
$54.4\%$ of the average execution energy across a variety of representative workloads for ULP devices.

\subsubsection{Specialization can limit programmability}
To combat the energy costs of generality, some recent work has turned to
microarchitectural specialization, making a system energy-efficient at the
expense of generality and
programmability~\cite{chen:isca16:eyeriss,chen:asplos14:diannao,du:isca15:shidiannao,liu:isca15:pudiannao,chen2014dadiannao,venkatesh2010conservation}.
% 
Specialization customizes a system's control and datapath to accommodate a
particular workload (e.g., deep neural networks~\cite{chen:isca16:eyeriss,chen:asplos14:diannao}),
eliminating inessential inefficiencies like instruction supply and RF access.
% 
The downsides of specialization are its high non-recurring engineering cost and its inability to support a wide range of applications.
% 
Given the emerging nature of applications in the ULP domain, specialization is premature, so new ULP, energy-efficient, but highly-programmable architectures are needed.

\subsubsection{Existing execution models are flawed}
Furthermore, both fixed-function ASIC designs and COTS scalar designs assume a trade-off between programmability and energy-efficiency that may be questionable.
% 
The fixed-function execution model of ASIC designs limits programmability, while the scalar execution model of COTS MCUs wastes significant energy.
% 
These execution models are at the extremes; there is room in the middle for alternative execution models that balance programmability and energy-efficiency.
% 
For example, one starting point is vector execution, which slightly reduces programmability, but amortizes instruction supply energy improving overall energy-efficiency.
% 
Developing new execution models, therefore, is critical to resolving the tension between programmability and energy-efficiency.

\subsubsection{Choosing the right programming interface}
Programmability needs to come in the correct form.
% 
It is more than just configurability.
% 
A design that's highly configurable, but difficult to program from a high-level language will have limited adoption.
% 
Legacy software needs to be supported out-of-the-box with minimal changes so that developers can quickly adopt new hardware.
% 
However there are downsides (e.g. incomplete information on memory-ordering and parallelism) to sticking with established programming interfaces.
% 
Therefore, choosing the right programming interface and building a compiler to target new hardware is as important as the hardware itself.


\figOverview

\section{Objective of this work}

This work proposes a complete system stack that leverages new execution models to maximize energy-efficiency without sacrificing programmability.
% 
This approach enables new applications in the ULP domain as improved energy efficiency makes sophisticated workloads practical, while maintaining support for programmability allows for iteration, development of new algorithms, and quick deployment. 
% 
My work fills out the stack -- from software to compilation to computer architecture and to silicon implementation.
% 
Together these works support the following thesis:

\begin{adjustwidth}{1cm}{1cm}
High energy-efficiency can be achieved 
% with new execution models integrated 
across the system stack from software to silicon without compromising on programmability by leveraging new execution models to reduce instruction and data supply energies.
% 

\end{adjustwidth}
% 
The following contributions form the basis for the thesis and the new system stack.

\begin{itemize}

\item[\textbf{[Software]}]
\textbf{\sonic is the first demonstration of DNN inference on energy-harvesting device (\autoref{chapter:sonic}): }

\sonic is an intermittence-aware software system with specialized support for DNN inference. 
% 
It runs optimized networks found using GENESIS, a tool that automatically compresses networks to balance inference accuracy and energy.
% 
\sonic introduces loop continuation, a new technique that dramatically reduces the cost of guaranteeing correct intermittent execution for loop-heavy code like DNN inference. 
% 
Across three neural networks on a commercially available MCU, \sonic reduces inference energy by $6.9\times$ over the state-of-the-art.
% 
\graham{add sentence saying why relates to thesis}

\item[\textbf{[Architecture]}]
\textbf{\manic is energy-efficient vector-dataflow co-processor (\autoref{chapter:manic}): }
\manic is an efficient vector-dataflow architecture for ultra-low-power embedded systems.  
% 
It achieves high energy-efficiency without sacrificing programmability and generality.
% 
\manic introduces \emph{vector-dataflow execution}, allowing it to exploit the
dataflows in a sequence of vector instructions and amortize instruction
fetch and decode over a whole vector of operations.
%
By forwarding values from producers to consumers, \manic avoids costly vector register file reads.
% 
By carefully scheduling code and avoiding dead register writes, \manic avoids costly vector register writes.
% 
On average, \manic is $2.8\times$ more energy efficient than a scalar baseline and $XX\%$ more energy-efficient than a vector baseline.
% 
\graham{add sentence saying why relates to thesis}

\item[\textbf{[Silicon]}]
\textbf{\manicsilicon proves the energy-efficiency of \manic (\autoref{manic:silicon}): }

\manicsilicon is a silicon prototype of the \manic architecture that demonstrates the energy-efficiency of the design.
% 
It is a complete, standalone system possessing a RISC-V scalar core, the \manic co-processor, a data cache, an instruction cache, and main memory composed of 1KB of bootloader ROM, 64KB of SRAM, and 256KB of non-volatile embedded MRAM.
% 
The design is implemented in Intel-16 (22nm) high-threshold-voltage process and achieves a max efficiency of 256 MOPS/mW drawing just 19$\mu$W at 4MHz.
% 
\graham{add sentence saying why relates to thesis}

\item[\textbf{[Architecture]}]
\textbf{\snafu generates ULP CGRAs (\autoref{chapter:snafu}): }

\snafu builds on \manic's vector-dataflow execution model, generating ULP coarse-grain reconfigurable arrays (CGRAs) that implement spatial-vector-dataflow execution.
% 
In spatial-vector-dataflow execution, a dataflow graph (DFG) is mapped spatially across the fabric of processing elements, applying the same DFG to many input data values, and routing intermediate values directly from producers to consumers.
% 
This minimizes instruction and data-movement energy, just like \manic, and eliminates unnecessary switching activity because operations do not share execution hardware.
% 
\snafu uses 41$\%$ less energy and runs 4.4$\times$ faster than \manic.
% 
\graham{add sentence saying why relates to thesis}

\item[\textbf{[Architecture \& Compilation]}]
\textbf{\riptide is an energy-minimal dataflow compiler \& CGRA architecture (\autoref{chapter:riptide}): }

\riptide rounds out the system stack, providing an ULP CGRA architecture and co-designed compiler that compiles high-level C-code to the new hardware.
% 
It proposes a new set of program primitives that support arbitrary control-flow, irregular memory accesses, common program idioms, and memory ordering without needing to tag values to save energy; this requires careful analysis by the compiler to guarantee correctness, particularly to enforce control-flow and memory ordering.
% 
To save even more energy, \riptide offloads control-flow operations into the on-chip network.
% 
\riptide observes that these operations are simple but prevelant, so it is wasteful to assign them to processing elements.
% 
Instead, \riptide reuses existing hardware in the on-chip network to directly implement control-flows operations.
% 
Compared to \snafu, \riptide uses 25\% less energy and is also XX\% faster without requiring hand-coded assembly.
% 
\graham{add sentence saying why relates to thesis}

\end{itemize}

Together \sonic, \manic, \manicsilicon, \snafu, and \riptide form the new energy-efficient system stack.
% 
By maintaining programmability while achieving state-of-the-art energy-efficiency these new systems will make deployments last longer, applications easier to develop, and enable more sophisticated processing on-device.
% 
In short, this new system stack will facilitate development of emerging applications in the ULP sensor domain.

\section{Outline}
This thesis is divided into eight chapters, including this introduction.
%
The second chapter provides useful background on embedded systems, intermittent computing, vector architectures, dataflow architectures, and coarse-grain reconfigurable arrays.
% 
The next four chapters describe \sonic, \manic \& \manicsilicon, \snafu, and \riptide in detail.
% 
The order of these chapters follows the timeline of development.
% 
This structure will make clear the connection between each work:
% 
\sonic, despite being the first to demonstrate DNN inference on an energy-harvesting device, exposed the drawbacks of existing ULP devices' scalar execution model; \manic solved these drawbacks with vector-dataflow execution; \snafu improved on \manic with spatial-vector-dataflow execution; and finally \riptide built on \snafu to further improve general-purpose programmability and overall system energy-efficiency.
% 
Finally, the last two chapters discuss exciting future research directions in ULP computer architecture and CGRAs and then conclude.
