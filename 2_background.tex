\chapter{Background}
\label{chapter:background}

This chapter discusses relevant prior work that this thesis builds on.
% 
The chapter is split into discussions on 1) low-powered embedded devices, 2) edge inference, 3) related efficient programmable architectures, and 4) coarse-grain reconfigurable arrays.
% , and 5) dataflow compilation.
% 
The first includes a general overview of ULP sensor devices and an introduction to intermittent computing.
% 
The second discusses algorithmic improvements and hardware accelerators for efficient neural network inference.
% 
The third describes relevant vector machine and dataflow architectures.
% 
And the fourth expands on a specific class of spatial-dataflow architectures, called coarse-grain reconfigurable arrays.
% 
% And the fifth describes prior work on dataflow/CGRA compilation.
% % 
% Lastly, there is a section that summarizes.

\figDevice
\section{Low-powered embedded devices}

Low-power embedded sensor devices as shown in \autoref{fig:device} are composed of: 1) low-powered sensors (e.g. HiMax HM01B0 camera~\cite{hm01b0}), 2) a low-powered radio (e.g. LoRaWAN~\cite{lorawan} or BLE~\cite{ble}), 3) an ULP microcontroller (e.g. ARM M0~\cite{cortexm0} or TI-MSP430~\cite{msp430fr5994}, 4) a power source like a coin-cell battery or capacitor, and 5) an optional energy harvester (e.g. solar cell or RF harvester~\cite{powercast}).
% 
% These devices can be deployed to remote environments and support relatively simple applications.
Energy dictates the viability of these devices.
% 
For battery-powered devices energy dictates lifetime, and for energy-harvesting devices that operate intermittently, energy dictates performance by controlling the time spent waiting for energy to be collected.
% 
On top of this, the MCUs of these are devices are simple, severely resource-constrained and energy-inefficient.

\subsection{Device operation}
% 
ULP sensor devices operate on a duty cycle: periodically sensors collect data from the environment, the microcontroller processes the data, and then data is transmitted via the low-power radio.
% 
This interval of data collection is dictated by application requirements (i.e. how often a particular event will take place), the required lifetime for the device (particularly for battery-powered devices), and energy-consumption of the different components of the device.
% 
% In other words, energy dictates the viability of these devices.
% 
Application requirements have to be carefully balanced with the energy consumption of the MCU, sensors, and radio.

\paragraph{Battery-backed systems}
Some devices rely on batteries~\cite{culler2002mica,jackson_2019,rowe2011sensor}.
% 
These devices can quickly deplete their battery even if the application is simplying logging data.
% 
Battery lifetimes can be improved by adding a energy-harvester, like a solar cell or radio-frequency harvester, that can recharge the battery.
% 
However, rechargeable batteries have limited recharge cycles and may not be able to cope with extreme environmental conditions (e.g. too hot/cold temperatures).

\paragraph{Capacitor-backed systems}
Instead a capacitor can be used to buffer energy from an energy harvester and power the device.
% 
Capacitors have an effective lifetime ($>$ 10 years) that often exceeds the lengths of application deployments.
% 
But they do not offer the same energy density as batteries.
% 
In a capacitor-backed system, the application often must wait for energy to be collected in the capacitor by the energy-harvester.
% 
This makes application performance dependent on the availability of energy in the environment.
% 
Energy availability is not constant --- input power can vary with environmental conditions.
% 
For example, weather and time-of-day significantly (by several orders of magnitude) impact the amount of energy a solar cell can harvest.

\subsection{Intermittent execution model}
\label{chapter:background:intermittent}
Systems that harvest energy and store that energy in a hardware-buffer (e.g. capacitor) usually operate intermittently.
% 
This is because device operating power usually exceeds power harvested from the environment.
% 
To operate despite this weak input power, a device slowly accumulates energy in a hardware buffer and operates when the buffer is full. 
% 
The device drains the buffer as it operates, then it turns off and waits for the buffer to fill again.
% 

Software executes in the {\em intermittent execution model} on these
energy-harvesting devices ~\cite{mementos,dino,dewdrop,quickrecall,idetic,jerger2017ehmodel}.
% 
In intermittent execution, software progresses in bursts, resetting
at frequent power failures.
% 
Existing devices~\cite{wolverine,msp430fr5994} mix volatile state (e.g., registers and SRAM) and non-volatile memory (e.g., FRAM). 
% 
A power failure clears volatile state while non-volatile memory persists.
%
Repeated power failures impede progress~\cite{mementos}, and may leave memory
inconsistent due to partially or repeatedly applied non-volatile memory
updates~\cite{dino}.
% 
These progress and consistency issues lead to incorrect
behavior that deviates from any continuously-powered execution~\cite{edb}.
% 
Specifically, write-after-read (WAR) dependences lead to inconsistent memory and differing control-flow as re-execution can expose the value from the latter write to the read, which is not possible in continuously-powered execution.

Prior work addressed progress and memory consistency using software
checkpoints~\cite{dino,ratchet,clank}, non-volatile processors (NVPs)~\cite{nvp,ma2017incidental},
and programming models based around atomic tasks~\cite{chain,alpaca,mayfly,alpaca}.
% 
Non-volatile processors may be a long-term solution, but require technology process changes, while software-checkpointing and task-based runtime systems can be deployed on existing devices.

\subsubsection{Checkpointing systems}
Checkpoint-based systems insert checkpoints into programs using compiler, runtime, and hardware support.
% 
Just-in-time (JIT) checkpointing is the most popular strategy.
% 
In JIT checkpointing, hardware monitors the voltage of the capacitor and when the voltage dips below a pre-determined threshold indicating a power failure is imminent, triggers an interrupt that checkpoints program state. 
% 
The interrupt writes back the volatile state of the program, including program counter and stack, to non-volatile memory.
% 
Then when power resumes, the runtime system restores the volatile state and jumps back to the place in execution where power failed.
% 
This system is often transparent to the programmer, but costs energy to save and restore state and may complicate (or even lead to wrong execution) the use of peripherals~\cite{surbatovich2021automatically} and interrupts.

\subsubsection{Task-based runtime systems}
\label{chapter:background:intermittent:task}
An alternative to checkpointing are task-based runtime systems.
% 
These systems avoid frequent checkpoints by
restarting from a task's start after power failure,
at which point all register and stack state must be re-initialized.
% 
To ensure memory consistency, tasks ensure that the effect of a
partial task execution is not visible to a subsequent re-execution.
Specifically, data that are read then written (i.e., a WAR dependence) may expose the result of an interrupted task.
% 
Task-based systems avoid ``the WAR problem'' with
redo-logging~\cite{alpaca} and static data duplication~\cite{chain}.


Task-based systems guarantee correct execution, but at a significant run-time cost.
% 
Redo-logging and static duplication both increase memory and compute
in proportion to the amount of data written.
% 
Transitioning from one task to the next takes time, so
short tasks that transition frequently suffer poor performance.
Long tasks better amortize transition costs,
but re-execute more work after a power failure.
% 
Worse, a task that is too long faces {\em non-termination} if the energy it
requires exceeds the energy that the device can buffer.
% 
The programmer, therefore, needs to be careful in splitting a program into atomic tasks.

\subsection{COTS ULP Devices}
In addition to being energy-constrained, ULP sensor devices are also severely resource constrained.
% 
ARM's Cortex M0~\cite{cortexm0} or TI's MSP430~\cite{msp430fr5994} are 
the most commonly used processors in existing ULP sensor
systems~\cite{wisp,capybara,flicker,ufop,amulet,wolverine}.
%
Such MCUs' frequency are typically 1--16MHz, leaving a
substantial performance gap compared to, e.g., a full-fledged, 2GHz Xeon-based
system.  
%
The MCU usually also houses all the memory available to
the system, including embedded SRAM, which is volatile, and embedded non-volatile memory (e.g. FRAM).  
%
Embedded memories are small and capacity varies by device. 
% 
A typical MSP430
low-power MCU includes just 1--4KB of SRAM and 32--256KB of FRAM.
% 
While
continuously powered (i.e. wired) embedded systems may interface with larger memories
via a serial bus ($I^{2}C$ or SPI), most ULP sensor devices do
not due to their high access energy and latency.
%
The typical operating power of an COTS ULP device is around 1--3mW.

\subsubsection{Architecture}
COTS ULP devices achieve ULP operation by being simple.
% 
They have an 3--5 stage, in-order scalar core, which may lack instruction and/or data caches.
% 
Some MCUs also come with a vector co-processor such as TI's Low Energy Accelerator (LEA) or support for vector extensions like Arm's Neon~\cite{neon} vector ISA.
% 
Additionally, some MCUs also include DMA engines and accelerators for tasks like AES encryption~\cite{msp430fr5994}.
% 
Despite these additional features, the energy consumption of the scalar core (and it's memory accesses) dominates total MCU energy.
% 
This is because the scalar execution model pays a high price for general-purpose programmability, constantly refetching and redecoding the same instructions and communicating intermediates via a centralized register file.
% 
The purpose of this thesis is to reduce this tax for general-purpose programmability.

\section{Edge inference}
As ULP sensor devices become pervasive they will increasingly need to make intelligent decisions.
% 
Deep neural network (DNN) inference is the state-of-the-art approach for such intelligence.
% 
They are the standard for applications ranging from understanding speech to image recognition~\cite{alexnet,vgg, googlenet}.
% 
With their accuracy, however, comes a high computational cost.
% 
Neural networks have millions or billions of parameters and require billions or even trillions of operations.
% 
This makes deploying neural networks are resource-constrained, energy-harvesting devices difficult.
% 
Fortunately there has been much work on reducing network footprint and improving the performance and energy-efficiency of inference.
% 
% Three strategies are of particular note: pruning, reduced precision and network design.

\subsection{Algorthmic improvements to NN inference}
\label{chapter:background:inference:algorithm}
Since DNNs are robust to noise, algorithmic optimizations can be made that reduce NN memory footprint and increase inference performance without significantly impacting accuracy.
% 
Inference does not need full-precision floating point~\cite{han:isca16:eie,desa:isca17:sgd} and near-zero weights can often be ``pruned''~\cite{nabhan1994toward, han:iclr16:deep-compression,nakkiran:interspeech15:compressing,bhattacharya2016sparsification} without losing much accuracy.
% 
Layers can also be factored or split into several smaller, less-computationally intense layers~\cite{szegedy2017inception,szegedy2016rethinking,chollet2016xception}.
% 
Finally, networks can be redesigned~\cite{iandola:arxiv16:squeezenet,tan2019efficientnet,sandler2018mobilenetv2} from the ground up to minimize storage and computation.
% 
These networks leverage smaller convolutional filters, but make up for accuracy degradation by being wider or deeper.
% 
This is worthwhile because there is a quadratic relationship between convolutional filter size (i.e. side length) and computational cost.

These algorithmic improvements sometimes come with modifications to the training regime to further increase accuracy.
% 
Networks can be fine-tuned during the final stages (final 20-30\% of epochs) of training to adapt to algorithmic changes.
% 
During fine-tuning, the forward direction (i.e. inference) adopts the algorthmic change (e.g. reduced precision or pruning), while the backward direction remains the same.
% 
This allows the network to adjust its weights to the algorithmic change.

More exotic training regimes have also been explored to enforce additional properties to help inference performance.
% 
Binary networks~\cite{lin2018binary} learn binary (-1, +1) weights that simplify multiplication to a single {\tt and} operation.
% 
Structured sparsity can also be enforced~\cite{deng2018permdnn,nakkiran:interspeech15:compressing,}.
% 
Pruning can lead to extremely sparse layers without much structure which can lead to irregular memory accesses.
% 
By enforcing structure on this sparsity during training, irregular memory accesses can be reduced.

\autoref{chapter:sonic} discusses the application of several algorithmic changes to compress neural networks to fit into device memory.
%
It describes a tool, called GENESIS, that prunes, factors, and reduces the precision of neural networks, fine-tuning network weights to improve accuracy.

\subsection{Inference accelerators}
The computer architecture community has also responded to the need for efficient DNN inference.
% 
Some architectures focus on dense computations~\cite{chen:isca16:eyeriss,
chen:asplos14:diannao, chen2014dadiannao}, others on sparse
computations~\cite{han:isca16:eie, du:isca15:shidiannao, maeri,
zhang2016cambricon}, and still others on CNN
acceleration~\cite{alwani2016fused,parashar:isca17:scnn, albericio2016cnvlutin, ding2017circnn, ren2017sc, song2018insitu}.  
%
Industry has followed this trend, embracing custom silicon for
DNNs~\cite{jouppi:isca17:tpu}.
% 
The key to the efficiency of these architectures is to maximize data reuse and
optimize data movement.
% 
For example, Eyeriss~\cite{chen:isca16:eyeriss} introduces row-stationary dataflow to maximize reuse on a spatial fabric, while MAERI~\cite{maeri} designs a new on-chip network specialized for DNN dataflows (including hard-to-accelerator LSTMS).

The circuits community has also responded, taping-out extremely specialized accelerators with orders of magnitude higher energy-efficiency.
%
These designs use low-level VLSI techniques (e.g. sub-threshold computing~\cite{fick2017subthresholdinference}), custom analog or mixed-signal circuits~\cite{bankman2018always}, or exploit emerging technologies (e.g. ReRAM crossbars~\cite{xue201924,xue202015}) to achieve multiple TOPS/W.
% 
Unfortunately this extreme energy-efficiency comes at the expense of programmability (and perhaps reliability in the case of analog circuits).
% 
In fact, some accelerators specialize to the particular NN architecture~\cite{bankman2018always} or even hard-code weights~\cite{something}.

The work described in \autoref{chapter:manic}, \autoref{chapter:snafu}, and \autoref{chapter:riptide} take a different approach and focus on entirely different power domain (mW v. $\mu$W).
% 
Energy-efficiency is achieved without sacrificing programmability.
% 
This makes these designs applicable to many different applications, future-proof to algorithmic developments, and reduces the upfront non-recurring engineering cost.

\section{Efficient programmable architectures}
There is a long history of computer architectures that increase performance and/or improve energy-efficiency while maintaining programmability.
% 
Specifically, prior work on vector and dataflow architectures inform the work of this thesis and specifically \manic, \snafu, and \riptide.
% 
% These architectures reduce or even eliminate instruction fetch and decode and can significantly reduce the cost of communicating data between operations.
% 
These architectures exploit structure in the program (among data and control-flow) to change the execution model to reduce or even eliminate instruction and data supply energies without sacrificing general-purpose programmability.

\subsection{Vector architectures}
Vector machines exploit data-parallelism to amortize instruction supply energy (i.e. fetch, decode, and issue) across many operations.
% 
Early vector machines targetted super-computing~\cite{cray_patent}, but most commercially available architectures (e.g., AVX~\cite{avx} and GPUs~\cite{fermi}) today support vectors.
% 
These vector designs target performance and operate at a power budget
orders-of-magnitude higher than that contributed by this thesis.
% 
Nonetheless, all vector designs require large vector register files (VRF)
exacerbating register file access cost, especially in designs that require a
VRF with many ports. 
% 
Thus, reducing VRF cost and complexity has been a primary focus of prior vector
designs~\cite{asanovic1996t0,kozyrakis2003overcoming}.

T0~\cite{asanovic1996t0,wawrzynek1996spert} is a vector architecture with
reconfigurable pipelines. 
% 
Software controls datapaths to chain operations,
eliminating VRF access within a chain.  
% 
However, microarchitectural details of
the datapath are exposed to software, requiring major software changes and
recompilation.

CODE~\cite{kozyrakis2003overcoming} reduces VRF cost by distributing the VRF
among heterogeneous functional units.
% 
This design is transparent to software 
because CODE renames operands at instruction issue
to use registers near an appropriate functional unit.
% 
Distribution lets CODE reduce VRF ports, but requires a routing network
to send values between functional units.

AVA~\cite{lazo2021adaptable} is a high-performance out-of-order vector processor that adapts the vector length and the number of vector registers stored in the VRF to the application.
% 
Some applications require long vectors, while others want moderate sizes.
% 
AVA trades additional vector length for fewer vector registers, spilling registers to memory as needed and then smartly pre-fetching them.
% 
Although AVA reduces energy and area, it's primary goal is performance (by supporting variable-sized vector lengths).

Finally, there has been much work on reducing the cost and improving the scalability of GPU register files.
% 
This includes virtualizing the RF~\cite{jeon2015gpu,vijaykumar2016zorua} so that physical registers can be shared, compressing registers~\cite{lee2015warped} to maximize utilization of the physical RF, and coalescing RF reads and writes~\cite{asghari2019corf} to minimize RF accesses. 
% 
Relative to the large (100s of KB), many-ported (32+ ports) register files of GPUs, these optimizations require minimal additional hardware, but do not scale to ULP domain where energy-efficiency is higher priority than performance.

\subsection{Dataflow architectures}
Dataflow architectures, like vector machines, also have a long history~\cite{dennis1975preliminary,dennis1980data,dennis1988efficient,ttda} that includes changes to the programming and execution model to eliminate control and data movement overheads.
% 
% Although pure dataflow architecture have not enjoyed the same commercial success as vector machines,
% 
In particular, dataflow is prevalent today as part out-of-order (OoO) execution engines (i.e., restricted
dataflow) that use \emph{operator fusion} to improve performance and reduce RF
pressure~\cite{bracy2004dataflow,sassone2004dynamic,kim2002instruction,sembrant2015long,acsiliouglu2015lazy}.
% 
However, pure dataflow architectures have not found the same commercial success, but spatial-dataflow architectures still show great promise for improving energy-efficiency and performance.

Dennis proposed the first dataflow architecture in 1975, introducing a small set of primitives to implement arbitrary control-flow by conditionally routing values to consumers.
% 
In 1990, the MIT tagged token dataflow machine showed how to practically implement dataflow in hardware.
% 
The design relies on the use of a domain-specific language, called Id~\cite{id-compiler}, to describe program dataflow and cap resource use to avoid resource-based deadlocks from unbounded parallelism.

Later, Wavescalar~\cite{swanson2003wavescalar} and Trips~\cite{trips} identified dataflow locality as the key determinant of sequential code performance.
% 
Wavescalar compiles C/C++ programs to WaveCache, a grid of simple compute units and memory that co-locates computation with data.
% 
Wavescalar handles arbitrary control-flow by tagging data (to distiguish instances of a value across loop iterations) and enforcing memory ordering by  converting memory dependences to data dependences during compilation.
% 
Despite preserving dataflow locality, it is clear that Wavescalar was not designed to minimize energy: the architecture relies on expensive tag-token matching and still fetches instructions, constantly reconfiguring compute pipelines.

Trips extracts hyperblocks (multiple basic blocks without backedges) from programs and executed these hyperblocks in dataflow-fashion across a mesh of processing elements.
% 
Trips heavily relies on speculation to increase performance; in-flight operations numbered in the hundreds or even thousands.
% 
This amount of speculation inevitibaly wastes energy by discarding mispeculated work.
% 
Further, Trips constantly reconfigures PE pipelines for each hyperblock, toggling control and data signals.

In contrast, ELM~\cite{balfour2008energy} is a dataflow architecture specifically designed for low-power, embedded operation. 
% 
ELM uses
restricted SIMD execution and operand forwarding to provide
dataflow-like execution.
% 
ELM's complex register file hierarchy and forwarding
mechanism are software-controlled, exposing microarchitectural details to the
programmer and requiring expert-level, hand-coded assembly for maximum
efficiency.
% 
Even with significant changes to the compiler toolchain, ELM poses
a risk of unpredictable performance and high programming cost. 

Since Wavescalar, Trips, and ELM there has been a resurgence of spatial-dataflow architectures because they offer better energy-efficiency and performance with lower hardware complexity.
% 
The next section goes into more detail.

\section{Coarse-grain reconfigurable arrays}
Spatial-dataflow architectures known as coarse-grain reconfigurable arrays (CGRAs) achieve high energy-efficiency and performance.
% 
A CGRA architecture~\cite{remarc,adres,matrix,dyser,revamp,opencgra,cgrame,wave,nguyen2021fifer,morphosys,mozart,ppa,fpca,plasticine,dadu2019towards,parashar2013triggered,capstan,nowatzki:isca17:stream-dataflow,goldstein2000piperench,weng2020dsagen,weng2020hybrid,voitsechov2014single,mishra2006tartan,tan2018stitch,karunaratne2017hycube,voitsechov2018inter,evx} is a spatial array of
processing elements (PEs) connected by an on-chip interconnect (NoC).
%
A PE in a CGRA consumes inputs and produces outputs 
consumed by another PE, forming a pipeline corresponding to 
program dataflow.
%
CGRA efficiency derives from avoiding control and data-movement overheads.
%
A CGRA reduces instruction overheads by mapping operations to a PE,
avoiding the need for instruction fetch and decode and simplifying control.
%
A CGRA mitigates data movement overheads by avoiding large register
files, instead moving operands through a NoC directly from 
producer PE to consumer PEs.

A wide variety of CGRA architectures target different domains.
%
CGRAs exist as standalone cores~\cite{trips,raw,swanson2003wavescalar,mishra2006tartan},
co-processors~\cite{tan2018stitch,hauser1997garp,beret,seed,adres,charm,camel,goldstein2000piperench},
components of a processor pipeline~\cite{dyser,dynaspam,chimera}
or memory hierarchy~\cite{livia},
or as accelerators~\cite{plasticine,wave,gorgon,capstan,q100,nowatzki:isca17:stream-dataflow,weng2020hybrid,dadu2019towards,polygraph,taskstream,voitsechov2014single,nguyen2021fifer,morphosys,ppa,fpca}.
%
These contexts expose a wide range of hardware design choices, including PE
operation set, PE complexity, and NoC topology.
% 
Further, PEs may be homogeneous or heterogeneous; the latter is more area- and
energy-efficient, but creates a combinatorically large design space~\cite{revamp}.
% 
PEs typically include functional units for arithmetic, logic,
and memory access, but can also include 
specialized functionality~\cite{snafu,weng2020dsagen,dadu2019towards,q100,gorgon,capstan,polygraph,taskstream}.
% 
Later chapters (~\autoref{chapter:snafu} and \autoref{chapter:riptide}) of this thesis will address CGRA design decisions in detail to maximize energy-efficiency.

\subsection{Types of CGRAs}
CGRA design can be categorized in four ways as identified by \cite{weng2020hybrid}.
% 
They are distinct by the way they schedule operations and whether PE resources are shared.
% 
Specifically the four types of CGRAs are: systolic (statically scheduled \& dedicated PEs), shared-systolic (statically scheduled \& PEs shared),  tagged-dataflow (dynamically schedule \& PEs shared), and ordered-dataflow (dynamically scheduled \& dedicated PEs).

\paragraph{Systolic}
Systolic designs~\cite{tartan,piperench,fpca,warp,nowatzki:isca17:stream-dataflow} rely on the compiler to schedule operations in space and time.
% 
These designs achieve high performance and energy-efficiency by eliminating dynamic control, but make compilation challenging.
% 
To maximize utilization, the compiler must reason about operation latencies, but these latencies might not be available at compilation time and might not be fixed (e.g. memory access latency depends on where data exists in the hierarchy).
% 
This limits the applications that can be mapped to these designs.

\paragraph{Shared-systolic}
Shared-systolic designs~\cite{remarc,adres,morphosys,matrix,karunaratne2017hycube} (\cite{weng2020hybrid} refers to these as ``CGRAs'') add additional layer of complexity to systolic designs.
% 
To maximize utilization of available hardware, the compiler generates schedules where operations can time-multiplex on the same PE resources.
% 
This makes scheduling even more challenging v. systolic designs.

\paragraph{Tagged-dataflow}
Tagged-dataflow designs~\cite{swanson2003wavescalarvoitsechov2014single,ttda,trips,parashar2013triggered} dynamically schedule operations in time (and potentially in space) and time-multiplex PE resources.
% 
These designs tag data tokens and then match tags to distiguish between multiple instances of a single value, dynamically firing/enabling an operation when tags of inputs match.
% 
This tagging mechanism maximizes performance, allowing speculation and the re-ordering of operations and even entire loop iterations.
% 
However, this sort of dynamic tracking of values requires a high power budget (hundreds of mW v. $\approx$1mW of ULP domain) and comes at a significant energy cost.

\paragraph{Ordered-dataflow}
Ordered-dataflow designs~\cite{snafu,plasticine,dyser} are the final category of CGRAs.
% 
They do not share PE resources and dynamically schedule operations in time, but disallow the re-ordering of tokens.
% 
This makes compilation easier (v. systolic and shared-systolic) since the compiler need not reason about operation latencies and is cheaper to implement in hardware (v. tagged-dataflow) since there is no need for tag matching (because tokens arrive in order).
% 
These benefits make ordered-dataflow a good choice for the ULP domain and are the reasons why \snafu (\autoref{chapter:snafu}) and \riptide (\autoref{chapter:riptide}) implement it.

\subsection{Low-power CGRAs}
The CAD and circuit communities~\cite{ipa,cma,srp,karunaratne2017hycube} have also contributed low-power CGRA designs.
% 
These designs are usually systolic or shared-systolic and operate at 10s of mW (still order of magnitude more than ULP domain).
% 
They use VLSI techniques to reduce power (e.g., low-voltage design, fine-grain clock/power gating) that are complementary to the CGRA designs (\snafu and \riptide) in this thesis.
%
This thesis focuses on {\em architecture} that minimizes energy and maintains flexibility.
%
In fact, \autoref{chapter:snafu} describes \snafu, which has a goal of letting designers generate ULP CGRAs at reduced VLSI effort.

\subsection{Compilation}
Compiling for CGRAs is challenging.
% 
Some architectures requires domain-specific languages~\cite{id,delite}, while others place constraints on the types fo programs that can be compiled (i.e. no outer loops or memory ordering is not enforced).
% 
This makes the compilation tractable, but also limits the applications supported.

Compilation for CGRAs is similar to hardware synthesis.
% 
The compiler must find a layout of
operations that fits within fabric resources with valid routes between
all producers and consumers.
%
In performance-focused CGRAs, the compiler must also reason
about timing to maximize utilization and minimize
initiation interval (minimum interval between subsequent loop iterations).
%
With this vast search space, optimization-based methods often
do not converge in a reasonable time~\cite{hybrid-sched,nowatzki2013general}.
%
Most CGRA compilers use heuristics~\cite{opencgra,revamp,park2008edge,mei2002dresc,dora,nowatzki2013general,hybrid-sched,weng2020dsagen} that can 
fail or produce poor mappings.
%
Recent work proposed graph convolutional networks as a
solution~\cite{mirhoseini2020chip}.
% 
This thesis will (\autoref{chapter:snafu} and \autoref{chapter:riptide}) describe an integer-linear programming approach to mapping a program to PE resources.
% 
This is tractable because the compiler does not need to reason about timing or
utilization for the proposed architectures (\snafu and \riptide).

\subsubsection{Dataflow control-flow models}
Compilation is also affected by the underlying control-flow model.
% 
There are three competing control-flow models for dataflow execution: predication, selection ($\phi$), and steering ($\phi^{-1}$).
% 
Each has benefits and drawbacks. 
% 
Steering is the best choice for an energy-constrained context because it avoids routing values to the not-taken branch paths, but it can require extra routing by the compiler.

\paragraph{Predication routes values unnecessarily}
Predication is popular, especially in GPU and vector
architectures~\cite{avx,hennessy2011computer}, converting conditional code to
straightline code to simplify execution.
% 
In predication, only one side of a branch fully executes while the other side partially executes, passing through results from the enabled side to downstream consumers.
%
Predication simplifies control flow in CGRAs because tokens arrive
on every path, simplifying operand ordering.
%
But predication has a performance and energy cost because values flow
unnecessarily through the not-taken path.
%
\snafu (\autoref{chapter:snafu}) uses predication for control flow, supporting simple, affine loops.

\paragraph{Selection ($\phi$) burns energy on paths not taken}
Selection executes both sides of a branch fully, sending results to a mux that
chooses between results using the branch decider.
%
Selection maximizes performance because the branch condition and each side of the branch execute speculatively in parallel.
%
Selection is common in recent CGRAs that focus on performance.
% 
However, selection wastes energy by throwing away work.

\paragraph{Steering ($\phi^{-1}$) is most energy efficient}
%
Steering was proposed in the original dataflow paper~\cite{dennis1975preliminary}
and has been used notably in a few dataflow architectures~\cite{swanson2003wavescalar,dataflow-a-complement,mishra2006tartan,beret,seed}.
% 
Steering routes values to only the taken path of a branch based on the branch's
decider.
%
Steering serializes execution of the taken path on the branch decider, but
avoids executing any operations on the not-taken path.
%
\riptide (\autoref{chapter:riptide}) implements steering to minimize energy,
as steering never fires unneeded operations. 

\figPower
\tabMotivate
\subsection{Compare \& contrast different CGRA designs}
\autoref{fig:power} and \autoref{tab:motivation} summarize the CGRA design space, comparing different CGRA designs to work of this thesis, \snafu and \riptide.
% 
In particuler, the figures highlight what makes the ULP sensor domain different.
% 
Specifically, most prior CGRAs target much higher power domains ($\approx$100\,mW~\cite{weng2020dsagen,karunaratne2017hycube,nowatzki:isca17:stream-dataflow,tan2018stitch,weng2020hybrid} or even up to 100\,W~\cite{plasticine,voitsechov2014single}),
and their design decisions do not translate well to the ULP domain.
%
The few CGRAs targeting ULP operation ($\approx$1\,mW)~\cite{ipa,cma,srp} are not flexible and leave energy savings on the table. 
% 
They places restrictictions on programs, e.g. supporting only simple, single-nested loops.

In contrast, this thesis presents \snafu and \riptide that are designed from the ground-up to minimize energy (even at the expense of area and performance) while maximizing flexibility.
% 
\snafu and \riptide, like DSAGEN~\cite{weng2020dsagen}, generate CGRAs, but instead of targetting performance, target ULP operation.
%
They minimize PE energy by statically assigning operations
to specific PEs and, unlike prior low-power
CGRAs~\cite{tan2018stitch,karunaratne2017hycube,ipa,cma,srp},
minimizes switching by not sharing PEs between operations.
%
Likewise, to minimize NoC energy, \snafu \& \riptide implement a statically configured, bufferless, multi-hop NoC,
similar to HyCube~\cite{karunaratne2017hycube}.
This NoC is a contrast with prior ULP CGRAs~\cite{cma,srp,ipa}
that restrict communication to a PE's immediate neighbors.
%
Unlike many prior CGRAs that are statically scheduled,
\snafu \& \riptide implements dynamic dataflow firing
to support variable latency FUs.
%
Dynamic dataflow firing is essential to \snafu \& \riptide's flexibility
and ability to support arbitrary, heterogeneous PEs in a single fabric.
%
\snafu \& \riptide avoids expensive tag-token matching~\cite{plasticine,dyser}
by disallowing out-of-order execution, unlike high-performance designs~\cite{ttda,parashar2013triggered,voitsechov2014single,swanson2003wavescalar}.
% 
Lastly, \riptide targets more than simple, vectorizable loops~\cite{karunaratne2017hycube,ipa,cma,srp}, compiling programs directly from C and supporting deeply-nested loops and irregular memory accesses.
%
The end result is that \snafu \& \riptide are flexible and general-purpose,
while still achieving extremely low operating power and high energy-efficiency.

