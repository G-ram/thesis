\chapter{Background}
\label{chapter:background}

This chapter discusses relevant prior work that this thesis builds on.
% 
The chapter is split into discussions on 1) low-powered embedded devices, 2) edge inference and 3) related programmable architectures.
% 
The first includes a general overview of ULP sensor devices and an introduction to intermittent computing.
% 
The second discusses algorithmic improvements and hardware accelerators for efficient neural network inference.
% 
The last includes discussion on relevant vector machines, dataflow architectures, and coarse-grain reconfigurable arrays.

\figDevice
\section{Low-powered embedded devices}

Low-power embedded sensor devices as shown in \autoref{fig:device} are composed of: 1) low-powered sensors (e.g. HiMax HM01B0 camera~\cite{hm01b0}), 2) a low-powered radio (e.g. LoRaWAN~\cite{lorawan} or BLE~\cite{ble}), 3) an ULP microcontroller (e.g. ARM M0~\cite{cortexm0} or TI-MSP430~\cite{msp430fr5994}, 4) a power source like a coin-cell battery or capacitor, and 5) an optional energy harvester (e.g. solar cell or RF harvester~\cite{powercast}).
% 
% These devices can be deployed to remote environments and support relatively simple applications.
Energy dictates the viability of these devices.
% 
For battery-powered devices energy dictates lifetime, and for energy-harvesting devices that operate intermittently, energy dictates performance by controlling the time spent waiting for energy to be collected.
% 
On top of this, the MCUs of these are devices are simple, severely resource-constrained and energy-inefficient.

\subsection{Device operation}
% 
ULP sensor devices operate on a duty cycle: periodically sensors collect data from the environment, the microcontroller processes the data, and then data is transmitted via the low-power radio.
% 
This interval of data collection is dictated by application requirements (i.e. how often a particular event will take place), the required lifetime for the device (particularly for battery-powered devices), and energy-consumption of the different components of the device.
% 
% In other words, energy dictates the viability of these devices.
% 
Application requirements have to be carefully balanced with the energy consumption of the MCU, sensors, and radio.

\paragraph{Battery-backed systems}
Some devices rely on batteries~\cite{culler2002mica,jackson_2019,rowe2011sensor}.
% 
These devices can quickly deplete their battery even if the application is simplying logging data.
% 
Battery lifetimes can be improved by adding a energy-harvester, like a solar cell or radio-frequency harvester, that can recharge the battery.
% 
However, rechargeable batteries have limited recharge cycles and may not be able to cope with extreme environmental conditions (e.g. too hot/cold temperatures).

\paragraph{Capacitor-backed systems}
Instead a capacitor can be used to buffer energy from an energy harvester and power the device.
% 
Capacitors have an effective lifetime ($>$ 10 years) that often exceeds the lengths of application deployments.
% 
But they do not offer the same energy density as batteries.
% 
In a capacitor-backed system, the application often must wait for energy to be collected in the capacitor by the energy-harvester.
% 
This makes application performance dependent on the availability of energy in the environment.
% 
Energy availability is not constant --- input power can vary with environmental conditions.
% 
For example, weather and time-of-day significantly (by several orders of magnitude) impact the amount of energy a solar cell can harvest.

\subsection{Intermittent execution model}
Systems that harvest energy and store that energy in a hardware-buffer (e.g. capacitor) usually operate intermittently.
% 
This is because device operating power usually exceeds power harvested from the environment.
% 
To operate despite this weak input power, a device slowly accumulates energy in a hardware buffer and operates when the buffer is full. 
% 
The device drains the buffer as it operates, then it turns off and waits for the buffer to fill again.
% 

Software executes in the {\em intermittent execution model} on these
energy-harvesting devices ~\cite{mementos,dino,dewdrop,quickrecall,idetic,jerger2017ehmodel}.
% 
In intermittent execution, software progresses in bursts, resetting
at frequent power failures.
% 
Existing devices~\cite{wolverine,msp430fr5994} mix volatile state (e.g., registers and SRAM) and non-volatile memory (e.g., FRAM). 
% 
A power failure clears volatile state while non-volatile memory persists.
%
Repeated power failures impede progress~\cite{mementos}, and may leave memory
inconsistent due to partially or repeatedly applied non-volatile memory
updates~\cite{dino}.
% 
These progress and consistency issues lead to incorrect
behavior that deviates from any continuously-powered execution~\cite{edb}.
% 
Specifically, write-after-read (WAR) dependences lead to inconsistent memory and differing control-flow as re-execution can expose the value from the latter write to the read, which is not possible in continuously-powered execution.

Prior work addressed progress and memory consistency using software
checkpoints~\cite{dino,ratchet,clank}, non-volatile processors (NVPs)~\cite{nvp,ma2017incidental},
and programming models based around atomic tasks~\cite{chain,alpaca,mayfly,alpaca}.
% 
Non-volatile processors may be a long-term solution, but require technology process changes, while software-checkpointing and task-based runtime systems can be deployed on existing devices.

\subsubsection{Checkpointing systems}
Checkpoint-based systems insert checkpoints into programs using compiler, runtime, and hardware support.
% 
Just-in-time (JIT) checkpointing is the most popular strategy.
% 
In JIT checkpointing, hardware monitors the voltage of the capacitor and when the voltage dips below a pre-determined threshold indicating a power failure is imminent, triggers an interrupt that checkpoints program state. 
% 
The interrupt writes back the volatile state of the program, including program counter and stack, to non-volatile memory.
% 
Then when power resumes, the runtime system restores the volatile state and jumps back to the place in execution where power failed.
% 
This system is often transparent to the programmer, but costs energy to save and restore state and may complicate (or even lead to wrong execution) the use of peripherals~\cite{surbatovich2021automatically} and interrupts.

\subsubsection{Task-based runtime systems}
An alternative to checkpointing are task-based runtime systems.
% 
These systems avoid frequent checkpoints by
restarting from a task's start after power failure,
at which point all register and stack state must be re-initialized.
% 
To ensure memory consistency, tasks ensure that the effect of a
partial task execution is not visible to a subsequent re-execution.
Specifically, data that are read then written (i.e., a WAR dependence) may expose the result of an interrupted task.
% 
Task-based systems avoid ``the WAR problem'' with
redo-logging~\cite{alpaca} and static data duplication~\cite{chain}.


Task-based systems guarantee correct execution, but at a significant run-time cost.
% 
Redo-logging and static duplication both increase memory and compute
in proportion to the amount of data written.
% 
Transitioning from one task to the next takes time, so
short tasks that transition frequently suffer poor performance.
Long tasks better amortize transition costs,
but re-execute more work after a power failure.
% 
Worse, a task that is too long faces {\em non-termination} if the energy it
requires exceeds the energy that the device can buffer.
% 
The programmer, therefore, needs to be careful in splitting a program into atomic tasks.

\subsection{COTS ULP Devices}
In addition to being energy-constrained, ULP sensor devices are also severely resource constrained.
% 
ARM's Cortex M0~\cite{cortexm0} or TI's MSP430~\cite{msp430fr5994} are 
the most commonly used processors in existing ULP sensor
systems~\cite{wisp,capybara,flicker,ufop,amulet,wolverine}.
%
Such MCUs' frequency are typically 1--16MHz, leaving a
substantial performance gap compared to, e.g., a full-fledged, 2GHz Xeon-based
system.  
%
The MCU usually also houses all the memory available to
the system, including embedded SRAM, which is volatile, and embedded non-volatile memory (e.g. FRAM).  
%
Embedded memories are small and capacity varies by device. 
% 
A typical MSP430
low-power MCU includes just 1--4KB of SRAM and 32--256KB of FRAM.
% 
While
continuously powered (i.e. wired) embedded systems may interface with larger memories
via a serial bus ($I^{2}C$ or SPI), most ULP sensor devices do
not due to their high access energy and latency.
%
The typical operating power of an COTS ULP device is around 1--3mW.

\subsubsection{Architecture}
COTS ULP devices achieve ULP operation by being simple.
% 
They have an 3--5 stage, in-order scalar core, which may lack instruction and/or data caches.
% 
Some MCUs also come with a vector co-processor such as TI's Low Energy Accelerator (LEA) or support for vector extensions like Arm's Neon~\cite{neon} vector ISA.
% 
Additionally, some MCUs also include DMA engines and accelerators for tasks like AES encryption~\cite{msp430fr5994}.
% 
Despite these additional features, the energy consumption of the scalar core (and it's memory accesses) dominates total MCU energy.
% 
This is because the scalar execution model pays a high price for general-purpose programmability, constantly refetching and redecoding the same instructions and communicating intermediates via a centralized register file.
% 
The purpose of this thesis is to reduce this tax for general-purpose programmability.

\section{Edge inference}
As ULP sensor devices become pervasive they will increasingly need to make intelligent decisions.
% 
Deep neural network (DNN) inference is the state-of-the-art approach for such intelligence.
% 
They are the standard for applications ranging from understanding speech to image recognition~\cite{alexnet,vgg, googlenet}.
% 
With their accuracy, however, comes a high computational cost.
% 
Neural networks have millions or billions of parameters and require billions or even trillions of operations.
% 
This makes deploying neural networks are resource-constrained devices difficult.
% 
Fortunately there has been much work on reducing network footprint and improving the performance and energy-efficiency of inference.
% 
% Three strategies are of particular note: pruning, reduced precision and network design.

\subsection{Algorthmic improvements to NN inference}
Since DNNs are robust to noise, algorithmic optimizations can be made that reduce NN memory footprint and increase inference performance without significantly impacting accuracy.
% 
Inference does not need full-precision floating point~\cite{han:isca16:eie,desa:isca17:sgd} and near-zero weights can often be ``pruned''~\cite{nabhan1994toward, han:iclr16:deep-compression,nakkiran:interspeech15:compressing,bhattacharya2016sparsification} without losing much accuracy.
% 
Layers can also be factored or split into several smaller, less-computationally intense layers~\cite{szegedy2017inception,szegedy2016rethinking,chollet2016xception}.
% 
Finally, networks can be redesigned~\cite{iandola:arxiv16:squeezenet,tan2019efficientnet,sandler2018mobilenetv2} from the ground up to minimize storage and computation.
% 
These networks leverage smaller convolutional filters, but make up for accuracy degradation by being wider or deeper.
% 
This is worthwhile because there is a quadratic relationship between convolutional filter size (i.e. side length) and computational cost.

These algorithmic improvements sometimes come with modifications to the training regime to further increase accuracy.
% 
Networks can be fine-tuned during the final stages (final 20-30\% of epochs) of training to adapt to algorithmic changes.
% 
During fine-tuning, the forward direction (i.e. inference) adopts the algorthmic change (e.g. reduced precision or pruning), while the backward direction remains the same.
% 
This allows the network to adjust its weights to the algorithmic change.

More exotic training regimes have also been explored to enforce additional properties to help inference performance.
% 
Binary networks~\cite{lin2018binary} learn binary (-1, +1) weights that simplify multiplication to a single {\tt and} operation.
% 
Structured sparsity can also be enforced~\cite{deng2018permdnn,nakkiran:interspeech15:compressing,}.
% 
Pruning can lead to extremely sparse layers without much structure which can lead to irregular memory accesses.
% 
By enforcing structure on this sparsity during training, irregular memory accesses can be reduced.

\autoref{chapter:sonic} discusses the application of several algorithmic changes to compress neural networks to fit into device memory.
%
It describes a tool, called GENESIS, that prunes, factors, and reduces the precision of neural networks, fine-tuning network weights to improve accuracy.

\subsection{Inference accelerators}
The computer architecture community has also responded to the need for efficient DNN inference.
% 
Some architectures focus on dense computations~\cite{chen:isca16:eyeriss,
chen:asplos14:diannao, chen2014dadiannao}, others on sparse
computations~\cite{han:isca16:eie, du:isca15:shidiannao, maeri,
zhang2016cambricon}, and still others on CNN
acceleration~\cite{alwani2016fused,parashar:isca17:scnn, albericio2016cnvlutin, ding2017circnn, ren2017sc, song2018insitu}.  
%
Industry has followed this trend, embracing custom silicon for
DNNs~\cite{jouppi:isca17:tpu}.
% 
The key to the efficiency of these architectures is to maximize data reuse and
optimize data movement.
% 
For example, Eyeriss~\cite{chen:isca16:eyeriss} introduces row-stationary dataflow to maximize reuse on a spatial fabric, while MAERI~\cite{maeri} designs a new on-chip network specialized for DNN dataflows (including hard-to-accelerator LSTMS).

The circuits community has also responded, taping-out extremely specialized accelerators with orders of magnitude higher energy-efficiency.
%
These designs use low-level VLSI techniques (e.g. sub-threshold computing~\cite{fick2017subthresholdinference}), custom analog or mixed-signal circuits~\cite{bankman2018always}, or exploit emerging technologies (e.g. ReRAM crossbars~\cite{xue201924,xue202015}) to achieve multiple TOPS/W.
% 
Unfortunately this extreme energy-efficiency comes at the expense of programmability (and perhaps reliability in the case of analog circuits).
% 
In fact, some accelerators specialize to the particular NN architecture~\cite{bankman2018always} or even hard-code weights~\cite{something}.

The work described in \autoref{chapter:manic,chapter:snafu,chapter:riptide} take a different approach and focus on entirely different power domain (mW v. $\mu$W).
% 
Energy-efficiency is achieved without sacrificing programmability.
% 
This makes these designs applicable to many different applications, future-proof to algorithmic developments, and reduces the upfront non-recurring engineering cost.

\section{Related programmable architectures}
There is a long history of computer architectures that increase performance (and improve energy-efficiency) while maintaining programmability.
% 
Specifically, prior work on vector and dataflow architectures inform the work of this thesis.
% 
% These architectures reduce or even eliminate instruction fetch and decode and can significantly reduce the cost of communicating data between operations.
% 
These architectures exploit structure in the program (among data and control-flow) to change the execution model to reduce or even eliminate instruction and data supply energies without sacrificing general-purpose programmability.

\subsection{Vector architectures}
Vector machines exploit data-parallelism to amortize instruction supply energy (i.e. fetch, decode, and issue) across many operations.
% 
Early vector machines targetted super-computing~\cite{cray_patent}, but most commercially available architectures (e.g., AVX~\cite{avx} and GPUs~\cite{fermi}) today support vectors.
% 
These vector designs target performance and operate at a power budget
orders-of-magnitude higher than that contributed by this thesis.
% 
Nonetheless, all vector designs require large vector register files (VRF)
exacerbating register file access cost, especially in designs that require a
VRF with many ports. 
% 
Thus, reducing VRF cost and complexity has been a primary focus of prior vector
designs~\cite{asanovic1996t0,kozyrakis2003overcoming}.

T0~\cite{asanovic1996t0,wawrzynek1996spert} is a vector architecture with
reconfigurable pipelines. 
% 
Software controls datapaths to chain operations,
eliminating VRF access within a chain.  
% 
However, microarchitectural details of
the datapath are exposed to software, requiring major software changes and
recompilation.

CODE~\cite{kozyrakis2003overcoming} reduces VRF cost by distributing the VRF
among heterogeneous functional units.
% 
This design is transparent to software 
because CODE renames operands at instruction issue
to use registers near an appropriate functional unit.
% 
Distribution lets CODE reduce VRF ports, but requires a routing network
to send values between functional units.

AVA~\cite{lazo2021adaptable} is a high-performance out-of-order vector processor that adapts the vector length and the number of vector registers stored in the VRF to the application.
% 
Some applications require long vectors, while others want moderate sizes.
% 
AVA trades additional vector length for fewer vector registers, spilling registers to memory as needed and then smartly pre-fetching them.
% 
Although AVA reduces energy and area, it's primary goal is performance (by supporting variable-sized vector lengths).

Finally, there has been much work on reducing the cost and improving the scalability of GPU register files.
% 
This includes virtualizing the RF~\cite{jeon2015gpu,vijaykumar2016zorua} so that physical registers can be shared, compressing registers~\cite{lee2015warped} to maximize utilization of the physical RF, and coalescing RF reads and writes~\cite{asghari2019corf} to minimize RF accesses. 
% 
Relative to the large (100s of KB), many-ported (32+ ports) register files of GPUs, these optimizations require relatively little additional hardware, but do not scale to ULP domain where energy-efficiency is higher prioriy than performance.

\subsection{Dataflow architectures}
Dataflow architectures, like vector machines, also have a long history~\cite{dennis1975preliminary,dennis1980data,dennis1988efficient,ttda} that includes changes to the programming and execution model to eliminate control and data movement overheads.
% 
% Dataflow is most prevelant as part out-of-order (OoO) execution engines (i.e., restricted
% dataflow)
% .uses \emph{operator fusion} to improve performance and reduce RF
% pressure~\cite{bracy2004dataflow,sassone2004dynamic,kim2002instruction,sembrant2015long,acsiliouglu2015lazy}.
 
Dennis proposed the first dataflow architecture in 1975, using a small set of primitives (e.g. merge, decider, and steer) to implement arbitrary control-flow.
% 
In 1990, the MIT tagged token dataflow machine showed how to practically implement dataflow in hardware.
% 
The design relied on the use of a domain-specific language, called Id~\cite{id-compiler}, to describe program dataflow and reason about resource use (specifically to avoid resource-based deadlocks from unbounded parallelism).
%
Later, Wavescalar~\cite{swanson2003wavescalar} and Trips~\cite{trips} identified dataflow locality as the key determinant of sequential code performance.
% 
Wavescalar compiled C/C++ programs to WaveCache, a grid of simple compute units and memory that co-located computation with data.
% 
Wavescalar handled arbitrary control-flow by tagging data and enforcing load-store ordering by converting memory dependences to data dependences.
% 
Trips extracted hyperblocks (multiple basic blocks without backedges) from programs and executed these hyperblocks in dataflow-fashion across a mesh of processing elements.
% 
Trips speculated on the next hyperblocks to run, increasing the number of in-flight operations to hundreds or even thousands.
% 
Although both Wavescalar and Trips achieved 

% , but neither was designed to maximize energy-efficiency 


\subsection{Coarse-grain reconfigurable arrays}
SGMF, Plasticine, Dyser, Revel, etc.
\subsubsection{ULP CGRAs}
\subsubsection{CGRA compilation}
