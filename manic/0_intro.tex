Tiny, pervasively deployed, ultra-low-power sensor systems
enable important new applications in environmental sensing, in- and on-body
medical implants, civil infrastructure monitors, and even tiny chip-scale
satellites.
% 
These applications require a new ULP sensor system stack.
% 
\autoref{chapter:sonic} contributed a software component to this new stack, describing how these devices can be made ``intelligent'' with on-device machine inference using \sonic.
% 
However, \sonic also demonstrated that existing systems suffer fundamental
inefficiencies that demand new, extremely energy-efficient computer
architectures. 

\paragraph{Sensing workloads are increasingly sophisticated} 
Sensor devices are collecting increasingly more data as sensor capability has matured.
% 
This increase in sensed data volume requires more sophisticated processing.
% 
But as \autoref{chapter:sonic} argued, offloading work to a more powerful edge device or to the cloud is impractical as transmitting data takes much more energy per byte than computing locally.
%
Under these constraints, application performance becomes dependent on the energy-efficiency of computation.
%

Energy-efficiency is only half the story, though.
% 
A computation-heavy sensor system also needs to be highly programmable to support a wide variety of applications.
% 
But, programmability and energy-efficiency are in tension, since programmability often carries a significant energy penalty.
% 
Our goal is to design a highly programmable architecture that \emph{hides microarchitectural complexity while eliminating the energy costs of programmability}.

\paragraph{Existing low-power architectures fall short}
While \autoref{chapter:sonic} showed that it is possible to run sophisticated processing on existing devices, ULP COTS MCUs (e.g., TI MSP430, ARM M0+ \& M4+) nonetheless fail to meet the criteria for effective sensor nodes.
% 
Their scalar execution models pay a high price for general purpose programmability
(see the \textit{COTS MCU} dot in~\autoref{fig:manic:intro}), wasting energy fetching \& decoding instructions, controlling execution pipeline resources, and supplying data.

\paragraph{Programming pitfalls of architectural specialization}
Specialization of a system's control or datapath is one way to reduce the tax of general-purpose programmability, eliminating inessential hardware structures and functions for a particular application.
% 
But this comes at the expense of flexibility and generality (see the \textit{ASIC} dot in~\autoref{fig:manic:intro}), making a highly specialized design susceptible to quick obsolescence.

\paragraph{Existing programmable, efficient designs are insufficient}
In contrast to specialization, another approach to programmable
energy-efficiency is to target a conventional vector architecture (such as
NVidia's Jetson TX2~\cite{jetsontx2}, ARM NEON~\cite{neon}, or TI LEA~\cite{lea}), amortizing the cost of instruction
supply across a large number of compute operations. Unfortunately,
vector architectures exacerbate the energy costs of RF access,
especially in high-throughput designs with multi-ported vector register files (VRFs)~\cite{kozyrakis2003overcoming,asanovic1996t0,iram},
and so remain far from the energy-efficiency of fully specialized designs~\cite{hameed2010understanding} (see the \textit{classic vector} dot in~\autoref{fig:manic:intro}).

The ELM architecture stands out among prior efforts as an architecture that
targets ultra-low-power operation, operates with extremely high
energy-efficiency, and retains general-purpose
programmability~\cite{balfour_elm_thesis,balfour2008energy}.
%
The key to ELM's efficiency is an \emph{operand forwarding} network that avoids
latching intermediate results and a distributed RF that provides
sufficient register storage, while avoiding unfavorable RF energy scaling. 
%
Unfortunately, despite these successes, ELM faces fundamental limitations that prevent its
widespread adoption.
%
ELM makes significant changes to the architecture and microarchitecture of the
system, requiring a full re-write of software to target its exotic,
software-managed RF hierarchy and instruction-register design.  This programming task
requires expert-level assembly hand-coding,
as compilers for ELM are unlikely to be simple or efficient;
e.g., ELM itself cites a nearly 2$\times$ drop in performance when moving from hand-coded assembly to compiler-generated assembly~\cite{balfour2008energy}.
%
While ELM supports general-purpose programs, it does so with a high
programmability cost and substantial changes to software development tools (as shown in~\autoref{fig:manic:intro}).

\figMANICDesignSpace

\paragraph{Our design and contributions}
This chapter presents \manic: an efficient vector-dataflow
architecture for ultra-low-power embedded systems.  
%
As depicted in~\autoref{fig:manic:intro}, \manic is closest to the \textit{Ideal} design, achieving high energy-efficiency while remaining general-purpose and simple to program.
%
%
\manic is simple to program because it exposes a standard vector ISA interface
based on the RISC-V vector extension~\cite{riscv_2019}.

\manic achieves high energy-efficiency by eliminating the two main costs of
programmability through its vector-dataflow design.
%
First, \textbf{vector} execution amortizes
instruction supply energy over a large number of operations.
%% like prior vector designs.
%
Second, \manic addresses the high cost of VRF accesses
through its \textbf{dataflow} component by forwarding operands 
directly between vector operations.
%
\manic transparently buffers vector outputs in a small {forwarding buffer}
and, at instruction issue, renames vector operands to directly access
the forwarding buffer, \emph{eliminating read accesses to the VRF}.
%
Additionally, \manic extends the vector ISA with \textbf{kill annotations} that denote
the last use of a vector register,
\emph{eliminating write accesses to the VRF}.
%
The vector-dataflow architecture is efficient because \manic amortizes the energy of tracking dataflow across many vector operations.
%
\manic thus eliminates a large fraction of VRF accesses (90.1\% on average in our experiments)
with simple microarchitectural changes that leave the basic vector architecture intact.

Finally, we have designed and implemented a code scheduling algorithm
that exploits \manic's operand forwarding to minimize VRF energy, while being \emph{microarchitecturally agnostic}.
In other words, it is \emph{not} necessary to expose the details
of the pipeline architecture or size of forwarding buffers
to minimize VRF energy---%
a single code schedule is near-optimal across a range of microarchitectural design points.

To evaluate \manic, we taped-out a test-chip in Intel's 22nm high-threshold voltage process that included \manic, a scalar design, and a vector design.
% We tape-out \manic in Intel's 22nm high-threshold voltage process and
We measured the energy of the various designs in the test-chip across a collection of programs appropriate to the deeply embedded domain.
%
\manic reduces energy by $3.4\times$ v. the scalar design and by $12\%$ v. the vector design.
% 
% Using post-synthesis energy estimates, we show that \manic
% is within $XX\%$ of the energy of an idealized design while remaining fully general and making few, unobtrusive changes to the ISA and software development stack.
%