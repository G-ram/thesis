\newcommand{\pes}{PEs\xspace}
\newcommand{\fend}{frontend\xspace}
\newcommand{\mend}{middle-end\xspace}
\newcommand{\bend}{backend\xspace}

\section{RipTide Compiler}
\label{sec:compiler}

\riptidecomp compiles, optimizes, and maps high-level C code to \riptide's CGRA fabric. 
%
%% The ability to compile high-level language code to a correct and highly
%% efficient \riptide configuration is a primary contribution of this work.
%
%\riptidecomp maps operations onto hardware, producing an application bitstream
%for the CGRA.
%
%% \subsection{Compilation pipeline and infrastructure}
%
%% \souradip{compiler figure needs to label frontend, middle-end, backend}
%
Its compiler has a \fend, \mend, and \bend.
%
The \fend uses clang to compile C to LLVM's intermediate representation (IR).
%
% The \mend optimizes the code in LLVM's IR, then translates to
% target-specific IR, represented as a dataflow graph (DFG) of the operator
% types from \autoref{sec:cf}, including control flow and memory ordering.  
% 
The \mend manipulates the LLVM IR to insert control-flow operators from  \autoref{sec:cf} and enforce memory ordering; then it translates the IR to a dataflow graph (DFG) representation and optimizes the DFG by transforming and fusing subgraphs, reducing operator count by 27\%. 
%
%The target-specific IR includes PE operations, but retains high-level
%information about control-flow.\brandon{anything besides cf and loops?}
%
%The target-specific IR is agnostic to PE hardware, their timing, and the CGRA topology.
%
%The \mend does memory order minimization, loop
%``streamification'', and operation fusion.
%
%% directly usable for configuring the CGRA.
The \bend takes the DFG as input and maps operators onto the CGRA,
producing a configuration bitstream in minutes.
% that is directly usable for configuring the CGRA.
%
\autoref{fig:compiler} illustrates the \mend's compiler passes. 
%
%We built the \fend and \mend using LLVM~\cite{llvm} and the \fend's
%target-independent IR is LLVM IR.

%\paragraph{Why start from C?}
%
%\riptide targets C to support existing code and to minimize programmer effort.
%
%C strikes a balance between programmability and efficient code 
%generation for the target architecture, and this is no different for \riptide.
%
%Compared to other high-level programming languages, compiling from C also 
%often results in a set of operations that is closer to the set representing 
%the ideal computation for any given program.
%
%\riptidecomp enjoy's LLVM's widespread adoption, existing analyses, and expressive
%IR. 
%
%Using LLVM to build \riptidecomp has two main advantages: 1) LLVM's wide array
%of compiler analyses produce accurate and powerful information about the 
%source code that is essential to generating efficient DFGs in \riptidecomp's \mend. 
%%
%2) By choosing LLVM-IR as the target-independent IR, \riptidecomp reuses an 
%extremely robust and portable infrastructure, making \riptide more accessible
%(and applicable to any frontend that supports LLVM-IR).
%
%\riptidecomp uses LLVM's control-flow graph (CFG) and loop normalization passes,
%alias analyses, scalar evolution analysis, and several other low-level
%analyses.
%
%\riptidecomp's IR builds on LLVM-IR, leveraging its ease of analysis, and adding
%\riptide-specific information as needed. \souradip{this sentence
%needs some depth and needs to be moved elsewhere} \brandon{still needs revision, but it's better now.} \brandon{What information are we adding to LLVM-IR in particular}
%
%\paragraph{Compilation Pipeline}
%\souradip{too much redundancy? details too early?}
%The \fend compiles C code to LLVM-IR using \texttt{clang}, optimizes the 
%application for code size (\texttt{-Oz}), and applies LLVM's normalization passes 
%(e.g. \texttt{-simplifycfg}, \texttt{-loop-simplify}, etc.).
%%
%The \mend then applies its custom compiler passes on the input LLVM-IR (in 
%order): 1) analyzing memory dependences and enforcing memory operation 
%ordering (load-store ordering, 2) inserting control-flow \pes, and 3) 
%fusing operations and streamifying, all of which incrementally produce an 
%efficient DFG of \pes for the application.
%%
%Finally, the \bend takes the DFG and a topology of the CGRA fabric to 
%schedule the DFG and produce a bitstream for the application.

\subsection{Memory-ordering analysis}
\label{sec:compiler:enforce}

\newcommand{\rem}[1]{\textcolor{darkgreen}{#1}}

\riptidecomp maps sequential code onto a CGRA fabric in which many operations,
including memory, may execute in parallel.  
%
For correctness, some memory operations must execute in a particular
order.
% 
% \riptidecomp's \mend uses alias analysis to compute required orderings between memory operations, encodes these orderings as arcs in an ordering graph (OG), optimizes the ordering graph and then adds control-flow operations according to enforce orderings.
\riptidecomp's \mend computes required orderings between memory operations present
in the IR and adds control-flow operations to enforce those orderings. 

%The compiler is given this responsibility in lieu of expensive and energy-inefficient 
%hardware that is required to perform dynamic memory diambiguation [?].
%\souradip{expand}. 
%
% The compiler uses alias analysis to identify dependent memory operations that
% may access the same memory locations and may execute on the \riptide fabric
% simultaneously, and hence must be ordered.

% \riptidecomp computes ordering relations by analyzing memory dependences.  
%
% A \textit{memory dependence} is an ordered relation from one memory operation
% (source) to another (destination) that access same memory location,
% where the destination is reachable from the source and one (or both)
% are writes.
%
% An \textit{ordering graph} (OG) is a digraph of the ordering relations
% required to ensure that parallel memory operations produce a
% result consistent with a sequential execution of the program.

\paragraph{Constructing a memory-operation ordering graph.} 
The first step to enforcing memory ordering is to construct an ordering graph (OG) that encodes dependences between memory operations.
% 
\riptidecomp uses alias analysis to identify memory operations
that may or must access the same memory location (i.e., alias), adding an arc between the operations in the OG accordingly.
%
\riptidecomp makes no assumptions on the alias analysis and
%% \riptidecomp can use a set of aliases produced by an arbitrary local alias 
%% analysis (flow-insensitive or flow-sensitive). 
%% \souradip{intraprocedural == single function, but we haven't mentioned
%% that each application is just a single function yet.} \brandon{app == function is not fundamental... does ordering break if an app is not a single function?}
%
% \riptide queries alias analysis for all memory operations and adds
% an edge to the OG for each pair that may or must alias.
%% The analysis considers every memory operation, querying alias
%% analysis for all aliasing memory operations.  The analysis adds an edge to the
%% OG for each pair of memory operations that may or must alias.
%
need not consider self-dependences because repeated instances of the
same memory operation are always ordered on its CGRA fabric. %mapped into \riptide's CGRA fabric are always ordered.
%\souradip{b/c no time multiplexing, but the \mend isn't supposed to be aware of that}
%
\autoref{fig:lso} shows a basic, unoptimized OG in the top left for an 
{\tt example} function.

\paragraph{Pruning the ordering graph.} 
The OG as computed can be greatly simplified.
% 
Prior work has simplified the OG with improved alias analysis~\cite{hind-aa} and by leveraging new control-flow primitives~\cite{midkiff-padua,doacross}.
% 
These efforts are orthogonal to \riptidecomp.
% 
\riptidecomp simplifies the OG by eliminating redundant
ordering arcs that are already enforced by data and control dependences.
%
% \rem{A data dependence orders a pair of operations in \riptide 
% because the destination will not execute before receiving an input token 
% produced by the source.}
% 
\riptidecomp finds data dependences by walking LLVM's definition-use (def-use)
chain from source to destination and removes \emph{ordering} arcs for dependent operations~\cite{nachos}.
%
For instance, in \texttt{example}'s CFG from \autoref{fig:compiler}, 
\textbf{S2} is data-dependent \textbf{L1}, so there need not
be an ordering arc in the OG. This is reflected in the blue-outlined arc
from \textbf{L1} to \textbf{S2} that is pruned in the OG in \autoref{fig:lso}.
%
%% Prior work~\cite{nachos} also removes ordering relations based on data dependences.
Similarly, control dependences order some memory operations if the execution of
the destination is control-dependent on the source.
%
%% If the branch governing the execution of the destination depends on the result
%% of the source operation, the control dependence entails ordering, because the
%% destination cannot execute in \riptide until the source completes, resolving the
%% branch.
%
\riptidecomp analyzes the CFG to identify control dependences
between memory operations and removes those orderings from the OG.
%
In \texttt{example}'s CFG from \autoref{fig:compiler}, the
arc from \textbf{L0} to \textbf{S1} in \autoref{fig:lso} is pruned using this analysis.

\figRipTideLSO

\paragraph{Transitive memory-ordering analysis.}
%We commonly observe \textit{transitive} ordering constraints in a function's
%OG [as shown in ...], where some edge, $e$ ($e_s$, $e_d$), is already ordered 
%(with respect to the OG) because there exists in the OG some alternative path $e_s$, 
%$e_{i0}$, ... $e_{in}$, $e_d$ to the destination. 
%\souradip{fix, this looks terrible.}
Two dependent memory operations are transitively ordered if there is a path (of
ordering arcs) in the OG from source to destination.
%
\riptidecomp finds and eliminates redundant arcs that are
transitively ordered by other control- and data-dependence orderings.
%
%The \mend first uses transitivity in the OG to identify more redundant 
%ordering constraints that are satisfied via data dependences or 
%data-dependent control-flow. 
%
%In particular, we select edges ($e_s$, $e_d$) that are transitive, 
%where there exists an acyclic path from $e_s$ to some intermediate node,
%$e_i$, in the OG, which $e_d$ depends on via data dependences or control dependences.
%
%In this scenario, $e$ is already satisfied because edge ($e_s$, $e_i$)
%will be explicitly ordered, while the ordering constraint ($e_i$, $e_d$) 
%is already satisfied in the existing code structure.
%
%We call this redundancy a \textit{transitive implicit ordering}.
%
%Transitive implicit orderings are handled the same as implicit orderings
%and pruned from the OG.
%
%\souradip{i'm introducing way too much terminology, really need to
%hammer down the explanations here, especially w/ cycles v.\ acyclic.}
%
This reduces the number of operations required to enforce ordering by $18\%$ v. unoptimized ordering.

To simplify its OG, \riptidecomp uses transitive reduction (TR)~\cite{aho-tr}, which prior work
deployed to simplify ordering relation graphs for parallel
execution of loops~\cite{midkiff-padua,compiler-sync}.
%
% Prior efforts also incorporated control flow to reduce ordering constraints~\cite{midkiff-padua,doacross} 
% or improve alias analysis accuracy to begin with~\cite{hind-aa}.
%
%After pruning, the ordering graph is simplified to a state 
%where all remaining edges exist only because of memory dependences.
%
%At this point, the \mend applies a transitive reduction[?] to the OG
%to optimize away remaining transitive ordering edges.
%
%Transitive reductions have been widely studied and applied in prior
%work to optimize redundant synchronization instructions in automatic
%parallelization techniques.
% \souradip{mention some prior work here? only in related work? i.e. "here's what padua did before" here?}
%
We apply TR to the OG, which converts a (potentially cyclic) ordering 
graph into an acyclic graph of strongly connected components (the SCCDAG). 
%
Traditional TR eliminates arcs between SCCs,
removes all arcs within each SCC,
and adds arcs to each SCC to form a simple cycle through all vertices.

We modify the algorithm in two ways to make it work for \riptide's OG.
First, arcs in the inserted cycle must be compatible with program order
instead of being arbitrary.  Second, the inserted arcs must
respect proper loop nesting, avoiding arcs directly from the inner to outer
loop.  To handle these arcs, we add synthetic loop entry and exit nodes to
each loop (shown as \texttt{src} and \texttt{sink} nodes at the bottom 
of \autoref{fig:lso}). Any arc inserted % in the condensed SCC 
that links an inner loop node to an outer loop node instead uses the inner 
loop's exit as its destination.
Symmetrically, an arc inserted that links an outer loop node to an inner loop
node has the inner loop's entry as its destination.   With these two changes,
the SCCDAG is usable for TR.
%
%The algorithm removes edges redundant with transitive paths in the
%graph\brandon{what is a transitive edge? i tried replacing with something that
%made more sense to me} to create a condensed graph. The analysis then expands
%each SCC by replacing its internal edges with a simple cycle through their internal
%nodes. \brandon{the last sentence does not parse.  what is it doing to simple
%cycles?}
%\souradip{it's removing all edges in the SCC and routing a single, simple cycle
%through the internal vertices}
%

However, we observe that applying existing TR analysis to the OG in \riptide fails to
preserve required ordering operations.
%
The problem is that a source and destination may be ordered along one
(transitive) path, and ordering along another (direct) path may be removed as
redundant.  Execution along the transitive path enforces ordering, but along the direct path does not, which is incorrect.
%
\autoref{fig:lso} shows a scenario where path-sensitivity is critical.
%
The path, \textbf{SCC3}(\textbf{S2})$\rightarrow$\textbf{SCC1}(\textbf{L0}), should not be eliminated 
in TR because the alternative path, \textbf{SCC3}(\textbf{S2})$\rightarrow$\textbf{SCC2}(\textbf{L1})$\rightarrow$\textbf{SCC1}(\textbf{L0}), does not  capture the direct control-flow path from \textbf{S2} to
\textbf{L0} via the backedge of the loop.
%
This problem arises due to \riptide's steering control and lack of a program
counter to order memory operations.

To correctly apply TR to remove redundant ordering arcs, \riptidecomp introduces
\emph{path-sensitive} TR,
%
which confirms that a transitive ordering path
subsumes all possible control-flow paths
before removing any ordering arc from the OG.
%% because of that transitive path. 
%
With this constraint in place,
% removed ordering arcs do not correspond to CFG
%paths that must enforce ordering, and
\riptide can safely use transitive reduction.

%\brandon{this sentence is a bit of a whopper}
%
%It is important to note that path-sensitivity does not need to
%be applied to the pruning optimizations because those redundant
%edges are already enforced via data and control dependences, both of
%which take control-flow and the code structure into account.
%

%\brandon{I'm not sure we have space for all of this.  I also do not understand it from skimming right now.  I am tired, so I will look again in the AM.  }
%\souradip{agreed, but i don't think we can say "we sequentialize the memory operations of
%the SCC" and be done with it}
%\indent \textit{Second}, \riptidecomp handles the "expansion" of SCCs differently
%once the appropriate transitive ordering edges are removed.
%%
%In particular, replacing an SCC with a simple cycle of ordering edges
%can be problematic because the direction of ordering constraints must
%be maintained; the source and destination of an ordering edge cannot
%be swapped.
%% 
%\souradip{realizing that I need more figs to describe this stuff ...}
%Instead, \riptidecomp maintains ordering constraints for the memory 
%operations involved in the SCC in \textit{program order}. 
%%
%Additionally, since the internal memory operations represented 
%in the SCC must reside in a loop nest in the CFG, the compiler 
%sequentializes the SCC's memory operations that reside in each loop. 
%%
%For each loop to consider, the compiler inserts two false nodes into 
%the LSO graph, \texttt{source} and \texttt{sink}, that represent the 
%loop header and latch respectively.
%\souradip{is latch the right terminology? i.e. basic block that is
%source of backedge}
%%
%All ordering edges in the SCC that are lexically backwards with respect 
%to program order are rerouted to the \texttt{sink} node, and a 
%single ordering edge is added from the \texttt{sink} to 
%the \texttt{source} node.
%\souradip{this can be confusing b/c "node insertion" doesn't mean
%some actual compilation technique, it's modifying the LSO graph. 
%this needs to be more clearly conveyed.}
%\souradip{it's also possible that source/sink are implementation 
%details and this misses the point of SCC sequentialization. if 
%that's the case, lso fig needs to change}
%%
%By transforming SCCs in this way, the \mend encodes program
%order into the OG and produces an "entry" and "exit" into each
%SCC.
%%
%This simplifies existing edges between SCCs by reducing 
%potentially several edges from internal memory operations from one
%SCC to another into a single edge between the "entry" and "exit" of
%two SCCs. 
%%
%\souradip{(ironically), text on SCC simplification needs to be simplified}


\paragraph{Enforcing ordering constraints.} 
Memory operators in \riptide 
produce a control token on completion and can optionally consume a
control token ($dep$ in~\autoref{tab:isa}) to enforce memory ordering.
%
The \mend encodes ordering arcs as defs and uses of data values in the IR
(as seen in the IR transform of loads and stores in \autoref{fig:compiler})
before lowering them as dependences in the DFG.
%
For a memory operator that must receive multiple control signals,
the \mend inserts order operations (\autoref{sec:cf}) to consolidate those signals.

\subsection{Control-flow operator insertion}

The compiler lowers its IR to use \riptide's control paradigm by inserting \riptide
control-flow operators into the DFG.
%

\paragraph{Steer.}
%% A steer gates a value until downstream consumers
%% are certain to execute.
%
The compiler uses the control dependence graph (CDG)~\cite{cytron} to 
insert steers.
%
For each consumer of a value, the compiler walks the CDG from the producer
to consumer
%% the node that
%% produces the value to the consumer's node.
and inserts a steer operator at
each node along the CDG traversal. 
%
%% An inserted steer takes a data input that is either the value or the output of
%% another steer operator inserted for the value in a parent node along the CDG
%% traversal, if one exists. 
% 
The steer's control input is the decider of the basic block that the steer depends on,
and its data input is the value or the output of an earlier inserted steer.
%

\paragraph{Carry and invariant.}
%% In a loop, a carry operator produces a
%% token for a loop-carried dependence and an invariant operator produces a token for
%% a loop-invariant value.
%
For loops, the compiler inserts a carry operator for loop-carried dependences
and an invariant operator for loop-invariant values
into the loop header.
%
A carry's data
input comes from the loop backedge that produces the value.  An invariant's
data input comes from the loop pre-header.
%
These operators should produce a token only if the next iteration of the loop is
certain to execute; to ensure this behavior, the compiler sets their control
signal to the decider of the block at the loop exit. 
%

\paragraph{Merge.}
%% An order operator ensures values produced by different loop iterations remain in
%% order.
%\souradip{figure 4 needs to show order gates, no?}
%The final component to enforce the control-paradigm in the DFG is
%to handle the possibility of cross-iteration reordering of data
%values when executing loops. 
%\souradip{this needs a figure, i think}
%
%In particular, there can exist basic blocks in the CFG that are
%a merge point for data values produced along different 
%control-flow paths to the merge point (for instance, values 
%propagating from the "then" and "else" sides of an if-then-else).
%\souradip{don't like "merge point" but using "phi node" or something
%is worse}
%%
%In a single loop iteration, only one value is guaranteed to propagate to
%a downstream consumer at the merge point, but this may not be the case
%during parallel execution of the loop in dataflow fashion.
%
If two iterations of a loop may take different control-flow
paths that converge at a single join point in the loop body, either may
produce a token to the join point first.
% 
But for correctness, the one from the
earlier iteration must produce the first token.
%
The compiler inserts a merge operator at a joint point in the CFG to ensure that 
tokens flow to the join point in iteration order.
%
The control signal {\tt D} for the merge operator is the decider of
nearest common dominator of the join point's predecessor basic blocks.
%
%% This decider determines control flow to the merge node. 
%
Since the earlier iteration sends its control signal first and \riptide does not reorder tokens, the merge operator effectively blocks the later iteration until the earlier iteration resolves. % from sending a control signal and token to the order operator
% until the earlier iteration completes. % and forwards its value token to the merge node.
%

\figRipTideArch

\subsection{Stream fusion}
\label{sec:compiler:stream}

\riptidecomp performs target-specific operator fusion on the DFG to
reduce required operations and routes by combining value {\em stream generators} with loop
control logic and address computation logic.
%
\riptide supports streams and applies them for the common case 
of a loop with an affine loop governing induction variable (LGIV).
%
A stream makes loop logic efficient by fusing the LGIV update and the loop
exit condition into a single operator.
%
In the DFG, loop iteration logic is represented by the exit condition, an
update operator, the carry for the LGIV's value, and the steer that
gates the LGIV in a loop iteration.
%
The \mend fuses these operators into a single stream operator and sets
the stream's initial, step, and bound values.
%(which can be a constant or 
%loop-invariant symbol) as the inputs to the stream. 
%
\autoref{fig:compiler} shows stream compilation, where
the operators for loop iteration logic (outlined in blue in the 
DFG) are fused into a stream operator.
%
\riptidecomp uses applies induction variable analysis~\cite{dragon,zima-scev} to 
find affine LGIVs. % and fuses them with loop logic.
%
%
%
%
%\souradip{discuss decoupled streams from memory? see bullet points in latex file}
%
%\begin{itemize} 
%    \item \riptide's support for streams is disjoint from its possible
%        use cases, such as address calculation for memory operations, making
%        it a generalized sequence generator that any dependent computation
%        can use.
%        %
%        Decoupling sequence generation simplifies its downstream 
%        dependent operations since those operations themselves need
%        not be explicitly streamed, reducing compilation and hardware
%        complexity. 
%        %
%        Instead, an \texttt{add} or \texttt{load} operation simply 
%        consumes each individual data value from the sequence without
%        needing to compile those operations as a "streamed add", etc.
%    \item \riptide supports different configurations of streams (comparators,
%        step operations, etc.).
%\end{itemize}
%
\riptidecomp also identifies address computations, maps these to an affine
stream if possible, and fuses the stream into the memory operator.
%

% \tabRipTideSATHelpers
% \begin{figure}[htb]
% \begin{lstlisting}[style=custompython]
% # Enforce compatibility
% add(~Mvn[where(Cvn == 0)]) # for nodes
% add(~Mel[where(Cel == 0)]) # for links
% # Every vertex is mapped to one node
% for v in vertices: add(exactly1(Mvn[v]))
% # No node can be used more than once
% for n in nodes: add(atmost1(Mvn[:,n]))
% for r in routers:
%   lr = or(Mel[:,where(Hlr[:,r] != 0)], axis=1)
%   rl = or(Mel[:,where(Hrl[r] != 0)], axis=1)
%   # Incoming flow to router matches outgoing flow
%   add(~rl | lr)
%   add(rl | ~lr)
%   # Each edge is mapped to a single router output
%   for e in edges:
%     add(atmost1(Mel[e,where(Hrl[r] != 0)]))
% for e in edges:
%   for n in nodes:
%     if n is CF: continue # Skip control-flow nodes
%     nl = or(Mel[e,where(Hnl[n] != 0)])
%     ln = or(Mel[e,where(Hln[:,n] != 0)])
%     # If vertex is mapped to node, then outgoing
%     #   edges need to be mapped to outgoing links
%     add(Mvn[src(e),n] | ~nl)
%     add(~Mvn[src(e),n] | nl)
%     # If vertex is mapped to node, then incoming 
%     #   edges need to be mapped to incoming links
%     add(Mvn[dst(e),n] | ~ln)
%     add(~Mvn[dst(e),n] | ln)
% for e1 in edges:
%   for e2 in edges:
%     # Skip if edges can share same link 
%     # (e.g. same source vertex & port)
%     if can_share(e1, e2): continue
%     # Incompatible edges can't share links
%     add(~Mel[e1] | ~Mel[e2])
% for n in nodes:
%   if n is not CF: continue # Skip non-CF nodes
%   c = or(Mvn[:,n])
%   nl = or(Mel[:,where(Hnl[n] != 0)], axis=1)
%   ln = or(Mel[:,where(Hln[:,n] != 0)], axis=1)
%   # No backedges to router w/ CF node
%   for l in where(Hln[:,n] != 0):
%     add(~Mel[:,l] | ln)
%   for e in edges:
%     # If node is not being used as for control-flow
%     #   then incoming flow must match outgoing flow
%     add(ln[e] | c | ~nl[e])
%     add(~ln[e] | c | nl[e])
%     add(~ln[e] | ~Mvn[src(e),n])
%     # If node is being used for control-flow,
%     #   then incoming and outgoing edges of vertex 
%     #   must be mapped to incoming and outgoing 
%     #   links of CF node 
%     add(nl[e] | ~Mvn[src(e),n])
%     add(ln[e] | ~Mvn[dst(e),n])
% \end{lstlisting}
% \caption{SAT formulation}
% \label{lst:sat}
% \end{figure}  
% \figRipTideSATEnergy

\subsection{Mapping DFGs to hardware}
\label{sec:compiler:map}
\riptidecomp's backend takes a DFG and a CGRA topology description and generates
scalar code to invoke \riptide and a bitstream to configure the \riptide fabric.
% 
This involves finding a mapping of DFG nodes and edges to PEs, control-flow modules (\autoref{sec:fin}), and links.
% 
Mapping can be difficult, and there is much prior work on heuristic methods
that trade mapping quality for compilation speed~\cite{karunaratne2018dnestmap,hamzeh2012epimap,hamzeh2014branch,amp2020,4dcgra,himap,lee2021ultra,chordmap,pathseeker}.
% 
\riptide has two advantages v.\ this prior work.
% 
First, \riptide only needs to schedule operations in space, not time.
%
Prior compilers unroll loops to identify the initiation interval,
increasing program size.
% 
Second, \riptide targets energy-efficiency, not performance.
% 
Rather than optimize for initiation interval, it need only focus on finding a valid solution, since leakage is insignificant. 

\riptide provides two complementary mappers: one based on boolean satisfiability (SAT) and another based on integer linear programming (ILP) that minimizes the average routing distance.
%
The SAT-based mapper runs quickly, taking <\,3\,min for our most complex benchmark,
%DFS (45 vertices and 81 edges),
whereas the ILP-based mapper yields 4.3\% avg.\ energy savings v.\ SAT (\autoref{sec:eval:compiler}).

\paragraph{Problem description.}
The constraints of the ILP and SAT formulations are similar (see~\autoref{sec:appendix} for a complete, formal description).
% , though the objective of the ILP formulation is to minimize the average routing distance.
%
The formulations ensure that every DFG is mapped to a hardware node, that every edge is mapped to a continuous route of hardware links, and that the inputs and outputs of a vertex match the incoming and outgoing links of a hardware node. 
% 
Further, they disallow the mapping of multiple DFG vertices to a single hardware node, the sharing of hardware links by multiple edges with different source vertices, and the mapping of an DFG edge through a control-flow module when a DFG vertex is mapped to that module.
% 
Together these are the necessary constraints to produce not only a valid mapping, but also a good mapping (SAT is close to ILP in terms of energy).
% 
% These constraints are necessary to find a valid solution
% 
% The ILP formulation minimizes the average routing distance subject to these constraints

% \paragraph{Finding a solution.}
% The ILP-based solver uses CVXPY~\cite{cvxpy} and Gurobi 9.5~\cite{gurobi} to find a solution, while the SAT-based solver finds a solution in three steps.
% % 
% First, the mapper generates a DIMACS CNF file containing the formulation's clauses.
% % 
% Then the mapper invokes CaDiCal~\cite{cadical} on the file to rewrite and simplify the problem.
% % 
% Finally, the mapper invokes a new parallel SAT solver developed concurrently and based on YalSAT~\cite{yalsat} to find a valid mapping.
% \graham{cite solver}

% \paragraph{SAT formulation.}
% \autoref{tab:map:vars} lists the inputs to the ILP-based and SAT-based mappers as well as the variables that they solve for.
% % 
% The goal of either mapper is to solve for $M_{vn}$ and $M_{el}$, which map a DFG's vertices to hardware PEs and CF-modules (hardware nodes) and a DFG's edges to hardware links, respectively.
% % 
% Matrices $C_{vn}$ and $C_{el}$ capture the compatibility of a DFG's vertex-to-hardware node (i.e., a memory operation must be mapped to a memory PE) and a DFG's edge-to-hardware link (to make sure ports match), respectively.
% % 
% The remaining matrices $H_{nl}$, $H_{ln}$, $H_{rl}$, $H_{lr}$, describe the topology of the CGRA fabric by specifying the connectedness of links to hardware nodes and routers.
% \nzb{This whole paragraph needs to move to the appendix. It's useless without the tables.}

% \paragraph{SAT formulation.}
% \autoref{tab:sat} lists the clauses of the SAT formulation.
% % 
% There are three steps to finding a solution.
% % The problem is solved in three steps.
% % 
% First, the mapper generates a DIMACS CNF file containing the formulation's clauses.
% % 
% Then the mapper invokes CaDiCal~\cite{cadical} on the file to rewrite and simplify the problem.
% % 
% Finally, the mapper invokes a new parallel SAT solver developed concurrently (by a separate research group) and based on YalSAT~\cite{yalsat} to find a valid mapping.
% \graham{cite solver}
% \nzb{Without the tables, this is too mechanical. We need a qualitative description of what the SAT instance looks like and then we can give the mechanics of running the tools.}
% % 
% % This SAT solver will be open source at the time of publication.

% \paragraph{ILP formulation.}
% The objective of the ILP-based mapper is to minimize the average routing distance subject to the constraints listed in~\autoref{tab:ilp}.
% % 
% We describe the ILP using CVXPY~\cite{cvxpy} and use Gurobi 9.5~\cite{gurobi} to find a solution.
% \nzb{Likewise?}

% \paragraph{Scalability.}
% \nzb{Again, I don't think we've ever evaluated scalability? Unless by that we simply mean performance v.\ size of DFG for the ten programs we have?}
% The scalability of mapping is a primary challenge in CGRA compilation~\cite{karunaratne2018dnestmap,hamzeh2012epimap,hamzeh2014branch,amp2020,4dcgra,himap,lee2021ultra,chordmap,pathseeker}.
% % 
% However, for \riptide it is a different story --- the SAT-based mapper, in particular, scales well, finding solutions to most benchmarks in under a minute and taking no more than three minutes in the most complex case
% (on {\tt dfs}, which has 45 ops and 81 edges).
% % 
% \riptide has two advantages v.\ prior work.
% % 
% First, \riptide's mappers only need to schedule operations in space, not time.
% % 
% \riptide considers fewer operations than prior work,
% which unrolls loops to identify the initiation interval,
% %
% \rem{while also supporting operations with variable latency.}\nzb{Relevance of variable latency is unclear?}
% % 
% Second, \riptide targets energy-efficiency, not performance.
% % 
% Rather than optimize for initiation interval, its mappers need only focus on finding a valid solution,
% since leakage is insignificant and good solutions seem to be dense (SAT and ILP produce solutions similar in energy).
% 
